# ETL Data Pipeline Project

End-to-end ETL pipeline built using PySpark and AWS for scalable, production-ready data processing.

## Architecture

The pipeline follows a **Bronzeâ€“Silverâ€“Gold** layered architecture:

- **Bronze Layer:**  
  Raw CSV data ingestion from multiple sources into a centralized repository. Handles missing values, type casting, and initial data validation.
  
- **Silver Layer:**  
  Cleansed, validated, and normalized Parquet data. Includes deduplication, null handling, and date/format standardization. Optimized for downstream analytics.

- **Gold Layer:**  
  Aggregated and analytics-ready CSV data. Metrics include total sales, total orders, average price per product/store/category. Can be directly consumed by BI tools or reporting systems.

## Tech Stack

- **Data Processing:** PySpark  
- **Storage:** AWS S3 (Bronze, Silver, Gold layers)  
- **Serverless Automation:** AWS Lambda for triggering ETL on S3 uploads  
- **Monitoring:** AWS CloudWatch logs and alerts  
- **CI/CD:** GitHub Actions for automated testing and deployment  
- **Local Development:** Python 3.x, Pandas, and PySpark  

## Features

- **Automated ETL Execution:**  
  Lambda triggers the Gold layer pipeline whenever new Silver layer data is uploaded to S3.
  
- **Data Quality & Validation:**  
  Ensures schema consistency, handles nulls, deduplicates records, and validates date and numeric formats.

- **Scalable Architecture:**  
  Bronzeâ€“Silverâ€“Gold layers enable incremental updates and separation of raw vs processed vs analytics-ready data.

- **Single CSV Output for Analytics:**  
  Gold layer produces a single, consolidated CSV ready for BI tools, dashboards, or reporting pipelines.

- **CI/CD Integration:**  
  Full GitHub Actions workflow to push, test, and deploy ETL scripts and Lambda functions automatically.

- **Cloud Monitoring & Logging:**  
  Tracks execution status and errors in CloudWatch, enabling quick debugging and alerting.

## How it Works

1. **Triggers**: Any push or pull request (PR) to the `master` branch triggers the workflow automatically.

2. **ETL Execution**: The pipeline runs the ETL layers sequentially:
   - Bronze â†’ Silver â†’ Gold

3. **S3 Upload**: 
   - Old files in the Gold S3 bucket are removed.
   - The final single CSV generated by the Gold layer is uploaded to the S3 bucket.

4. **Lambda Trigger**: 
   - After the upload, the specified AWS Lambda function is invoked.
   - Logs for the Lambda execution can be monitored in AWS CloudWatch.

5. **Secrets**: 
   - GitHub Secrets must be configured for secure access to AWS:
     - `AWS_ACCESS_KEY_ID`
     - `AWS_SECRET_ACCESS_KEY`
     - `AWS_REGION`

---

### ðŸ”¹ Next Steps to Make It Work

1. **Add AWS Secrets in GitHub**:
   - Go to your repository â†’ Settings â†’ Secrets â†’ Actions â†’ New repository secret.
   - Add the following secrets:
     - `AWS_ACCESS_KEY_ID`
     - `AWS_SECRET_ACCESS_KEY`
     - `AWS_REGION`

2. **Update the GitHub Actions YAML**:
   - Replace `S3_BUCKET` with your Gold S3 bucket name.
   - Replace `LAMBDA_FUNCTION` with your Lambda function name.

3. **Push Workflow to Master**:
   - Commit and push your `.github/workflows/etl-ci-cd.yml` file to the `master` branch.
   - GitHub Actions will automatically:
     1. Run the ETL scripts.
     2. Upload the Gold CSV to S3.
     3. Trigger the Lambda function.




